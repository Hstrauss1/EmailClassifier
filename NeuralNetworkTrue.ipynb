{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1e9bcd-57d4-4831-9d38-bc93800a7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # strip URLs\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)           # strip punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # collapse whitespace\n",
    "\n",
    "    tokens = [\n",
    "        stemmer.stem(word)\n",
    "        for word in text.split()\n",
    "        if word not in STOPWORDS\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16ed634-e7b4-49b3-8e22-dfa97cfa64b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def load_emails(maildir: str):\n",
    "    texts, labels = [], []\n",
    "    cnt=0\n",
    "    for user in os.listdir(maildir):\n",
    "        user_dir = os.path.join(maildir, user)\n",
    "        if not os.path.isdir(user_dir):\n",
    "            continue\n",
    "\n",
    "        # only look at the “sent_items” folder\n",
    "        folder_dir = os.path.join(user_dir, \"sent_items\")\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        if not os.path.isdir(folder_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder_dir):\n",
    "            path = os.path.join(folder_dir, fname)\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    msg = BytesParser(policy=policy.default).parse(f) #read binary\n",
    "                body = msg.get_body(preferencelist=('plain',))\n",
    "                if body is None:\n",
    "                    continue\n",
    "                raw = body.get_content()\n",
    "                text = clean_text(raw)\n",
    "                if not text:\n",
    "                    continue\n",
    "                tokens = [w for w in text.split() if w not in STOPWORDS]  #cleanning. Added lemmentatizn or stemming?\n",
    "                texts.append(' '.join(tokens))\n",
    "                labels.append(user)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return texts, labels\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a34a5-5b42-433a-8f79-72651a3f50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning emails…\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/WAVE/projects/CSEN-140-Sp25/HHJ140Proj/Sent_Items_only\"     # path to Enron stuf\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 36\n",
    "\n",
    "print(\"Loading and cleaning emails…\")\n",
    "texts, labels = load_emails(DATA_DIR)\n",
    "print(f\"→ {len(texts)} messages from {len(set(labels))} authors\")\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018d0c61-5d9c-4d6e-a4b6-4d245ca9acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing with TF–IDF…\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing with TF–IDF…\")\n",
    "vect = TfidfVectorizer(max_features=20_000)\n",
    "X = vect.fit_transform(texts)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914d64f1-689b-453d-9e63-793ca6a13686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on 26364 docs; testing on 11299\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(labels)\n",
    "\n",
    "keep = {lbl for lbl, cnt in counts.items() if cnt >= 2}\n",
    "\n",
    "texts_filt = [t for t, l in zip(texts, labels) if l in keep]\n",
    "labels_filt = [l for l in labels if l in keep]\n",
    "\n",
    "X = vect.transform(texts_filt)   \n",
    "y = labels_filt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #split and train\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"→ Training on {X_train.shape[0]} docs; testing on {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08705ff9-5400-4318-8b98-add5da8dd86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Statistics:\n",
      "\n",
      "Training Set Size: 26364\n",
      "Test Set Size: 11299\n",
      "Number of Features (TF–IDF): 20000\n",
      "Number of Unique Authors (Labels): 135\n",
      "\n",
      "Top 10 Authors in Training Set:\n",
      "kaminski-v: 1161 emails\n",
      "dasovich-j: 1004 emails\n",
      "steffes-j: 965 emails\n",
      "germany-c: 935 emails\n",
      "kitchen-l: 790 emails\n",
      "watson-k: 677 emails\n",
      "presto-k: 669 emails\n",
      "blair-l: 650 emails\n",
      "heard-m: 547 emails\n",
      "lenhart-m: 536 emails\n",
      "\n",
      "TF–IDF Matrix Sparsity: 0.9961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Data Statistics:\")\n",
    "\n",
    "# Label distribution\n",
    "train_label_counts = Counter(y_train)\n",
    "test_label_counts = Counter(y_test)\n",
    "\n",
    "print(f\"\\nTraining Set Size: {X_train.shape[0]}\")\n",
    "print(f\"Test Set Size: {X_test.shape[0]}\")\n",
    "print(f\"Number of Features (TF–IDF): {X_train.shape[1]}\")\n",
    "print(f\"Number of Unique Authors (Labels): {len(set(y_train))}\")\n",
    "\n",
    "print(\"\\nTop 10 Authors in Training Set:\")\n",
    "for author, count in train_label_counts.most_common(10):\n",
    "    print(f\"{author}: {count} emails\")\n",
    "\n",
    "# Sparsity\n",
    "nonzero_elements = X_train.count_nonzero()\n",
    "total_elements = X_train.shape[0] * X_train.shape[1]\n",
    "sparsity = 1.0 - (nonzero_elements / total_elements)\n",
    "print(f\"\\nTF–IDF Matrix Sparsity: {sparsity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5595b2c6-ff2e-431d-950e-c67c0d9445bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k nearest neighbors k=1\n",
      "fitted\n",
      "0.4454376493495\n",
      "\n",
      "k nearest neighbors k=2\n",
      "fitted\n",
      "0.37206832463049827\n",
      "\n",
      "k nearest neighbors k=3\n",
      "fitted\n",
      "0.3494999557482963\n",
      "\n",
      "k nearest neighbors k=4\n",
      "fitted\n",
      "0.3377290025665988\n",
      "\n",
      "k nearest neighbors k=5\n",
      "fitted\n",
      "0.32967519249491106\n",
      "\n",
      "k nearest neighbors k=6\n",
      "fitted\n",
      "0.3244534914594212\n",
      "\n",
      "k nearest neighbors k=7\n",
      "fitted\n",
      "0.31914328701655015\n",
      "\n",
      "k nearest neighbors k=8\n",
      "fitted\n",
      "0.3155146473139216\n",
      "\n",
      "k nearest neighbors k=9\n",
      "fitted\n",
      "0.3098504292415258\n",
      "\n",
      "k nearest neighbors k=10\n",
      "fitted\n",
      "0.3048057350207983\n",
      "\n",
      "k nearest neighbors k=11\n",
      "fitted\n",
      "0.30153110894769447\n",
      "\n",
      "k nearest neighbors k=12\n",
      "fitted\n",
      "0.29763695902292236\n",
      "\n",
      "k nearest neighbors k=13\n",
      "fitted\n",
      "0.2930347818391008\n",
      "\n",
      "k nearest neighbors k=14\n",
      "fitted\n",
      "0.2886981148774228\n",
      "\n",
      "k nearest neighbors k=15\n",
      "fitted\n",
      "0.2856889990264625\n",
      "\n",
      "k nearest neighbors k=16\n",
      "fitted\n",
      "0.2829453933976458\n",
      "\n",
      "k nearest neighbors k=17\n",
      "fitted\n",
      "0.2807328082131162\n",
      "\n",
      "k nearest neighbors k=18\n",
      "fitted\n",
      "0.2807328082131162\n",
      "\n",
      "k nearest neighbors k=19\n",
      "fitted\n",
      "0.27940525710239844\n",
      "\n",
      "k nearest neighbors k=20\n",
      "fitted\n",
      "0.27781219576953714\n",
      "\n",
      "k nearest neighbors k=21\n",
      "fitted\n",
      "0.27621913443667584\n",
      "\n",
      "k nearest neighbors k=22\n",
      "fitted\n",
      "0.27391804584476503\n",
      "\n",
      "k nearest neighbors k=23\n",
      "fitted\n",
      "0.2748030799185769\n",
      "\n",
      "k nearest neighbors k=24\n",
      "fitted\n",
      "0.294096822727675\n",
      "\n",
      "k nearest neighbors k=25\n",
      "fitted\n",
      "0.38888397203292324\n",
      "\n",
      "k nearest neighbors k=26\n",
      "fitted\n",
      "0.4917249314098593\n",
      "\n",
      "k nearest neighbors k=27\n",
      "fitted\n",
      "0.5339410567306841\n",
      "\n",
      "k nearest neighbors k=28\n",
      "fitted\n",
      "0.5545623506505001\n",
      "\n",
      "k nearest neighbors k=29\n",
      "fitted\n",
      "0.5699619435348261\n",
      "\n",
      "k nearest neighbors k=30\n",
      "fitted\n",
      "0.5789892910877069\n",
      "\n",
      "k nearest neighbors k=31\n",
      "fitted\n",
      "0.5863350739003451\n",
      "\n",
      "k nearest neighbors k=32\n",
      "fitted\n",
      "0.5890786795291618\n",
      "\n",
      "k nearest neighbors k=33\n",
      "fitted\n",
      "0.5919107885653597\n",
      "\n",
      "k nearest neighbors k=34\n",
      "fitted\n",
      "0.5960704487122754\n",
      "\n",
      "k nearest neighbors k=35\n",
      "fitted\n",
      "0.5989025577484733\n",
      "\n",
      "k nearest neighbors k=36\n",
      "fitted\n",
      "0.60014160545181\n",
      "\n",
      "k nearest neighbors k=37\n",
      "fitted\n",
      "0.60014160545181\n",
      "\n",
      "k nearest neighbors k=38\n",
      "fitted\n",
      "0.6033277281175325\n",
      "\n",
      "k nearest neighbors k=39\n",
      "fitted\n",
      "0.6039472519692008\n",
      "\n",
      "k nearest neighbors k=40\n",
      "fitted\n",
      "0.6042127621913443\n",
      "\n",
      "k nearest neighbors k=41\n",
      "fitted\n",
      "0.6041242587839631\n",
      "\n",
      "k nearest neighbors k=42\n",
      "fitted\n",
      "0.6055403133020621\n",
      "\n",
      "k nearest neighbors k=43\n",
      "fitted\n",
      "0.6053633064872997\n",
      "\n",
      "k nearest neighbors k=44\n",
      "fitted\n",
      "0.6065138507832551\n",
      "\n",
      "k nearest neighbors k=45\n",
      "fitted\n",
      "0.6055403133020621\n",
      "\n",
      "k nearest neighbors k=46\n",
      "fitted\n",
      "0.6061598371537305\n",
      "\n",
      "k nearest neighbors k=47\n",
      "fitted\n",
      "0.6055403133020621\n",
      "\n",
      "k nearest neighbors k=48\n",
      "fitted\n",
      "0.6081954155234977\n",
      "\n",
      "k nearest neighbors k=49\n",
      "fitted\n",
      "0.606425347375874\n"
     ]
    }
   ],
   "source": [
    "#These are the models JoJo is testing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "models = {\n",
    "}\n",
    "\n",
    "#modelsButPair =[()]\n",
    "for i in range(1,50):\n",
    "    kNumber= \"k nearest neighbors k=\" + str(i)\n",
    "    models[kNumber]=KNeighborsClassifier(n_neighbors=i)\n",
    "    #modelsButPair[i-1]=zip(i,KNeighborsClassifier(n_neighbors=i))\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(accuracy_score(y_test, preds))\n",
    "    #print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4952d2c-8208-4fb4-8595-b6c45c760545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes\n",
      "fitted\n",
      "0.39667227188246745\n",
      "\n",
      "Logistic Regression 4000\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 2000\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 1000\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 500\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 250\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 125\n",
      "fitted\n",
      "0.7487388264448182\n",
      "\n",
      "Logistic Regression 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/jtorresmolina/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted\n",
      "0.7482963094079123\n",
      "\n",
      "Logistic Regression 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/jtorresmolina/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted\n",
      "0.7296220904504823\n",
      "\n",
      "Decision Tree\n",
      "fitted\n",
      "0.3550756704133109\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(alpha=1.0),\n",
    "    \"Logistic Regression 4000\": LogisticRegression(max_iter=4000),\n",
    "    \"Logistic Regression 2000\": LogisticRegression(max_iter=2000),\n",
    "    \"Logistic Regression 1000\": LogisticRegression(max_iter=1000),\n",
    "    \"Logistic Regression 500\": LogisticRegression(max_iter=500),\n",
    "    \"Logistic Regression 250\": LogisticRegression(max_iter=250),\n",
    "    \"Logistic Regression 125\": LogisticRegression(max_iter=125),\n",
    "    \"Logistic Regression 62\": LogisticRegression(max_iter=62),\n",
    "    \"Logistic Regression 31\": LogisticRegression(max_iter=31),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=20)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(accuracy_score(y_test, preds))\n",
    "    #print(classification_report(y_test, preds))\n",
    "    #print(\"Confusion matrix:\")\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109c359b-7bc3-430d-a189-21fbcb7b6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistical regression is the best. Preform For the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b42e65a-d811-41c4-8676-414ef0078eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression 100000000\n",
      "fitted\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.90      0.68      0.77       103\n",
      "       arnold-j       0.69      0.69      0.69       216\n",
      "        arora-h       1.00      0.30      0.46        20\n",
      "       badeer-r       0.00      0.00      0.00         2\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.79      0.66      0.72        73\n",
      "     baughman-d       0.93      0.46      0.62        28\n",
      "         beck-s       0.84      0.90      0.87       145\n",
      "       benson-r       0.00      0.00      0.00         5\n",
      "        blair-l       0.90      0.96      0.93       278\n",
      "      brawner-s       0.83      0.23      0.36        22\n",
      "          buy-r       0.91      0.82      0.86       117\n",
      "     campbell-l       0.81      0.50      0.62        34\n",
      "       carson-m       0.96      0.68      0.79        34\n",
      "         cash-m       0.87      0.87      0.87       164\n",
      "    causholli-m       0.98      0.94      0.96        67\n",
      "       corman-s       0.78      0.70      0.73       188\n",
      "     crandell-s       0.90      0.61      0.73        44\n",
      "       cuilla-m       1.00      0.44      0.61        41\n",
      "     dasovich-j       0.74      0.92      0.82       430\n",
      "        davis-d       1.00      0.43      0.60        14\n",
      "         dean-c       0.00      0.00      0.00        10\n",
      "     delainey-d       1.00      0.06      0.11        18\n",
      "      derrick-j       0.78      0.91      0.84       175\n",
      "       donoho-l       0.89      0.60      0.72        65\n",
      "      donohoe-t       0.00      0.00      0.00         9\n",
      "      dorland-c       0.77      0.86      0.81       176\n",
      "        ermis-f       0.00      0.00      0.00         5\n",
      "       farmer-d       0.94      0.64      0.76        77\n",
      "      fischer-m       1.00      0.32      0.48        19\n",
      "       forney-j       0.97      0.95      0.96       128\n",
      "         gang-l       1.00      0.70      0.83        27\n",
      "          gay-r       0.00      0.00      0.00         3\n",
      "     geaccone-t       0.85      0.69      0.76       109\n",
      "      germany-c       0.43      0.91      0.59       401\n",
      " gilbertsmith-d       0.00      0.00      0.00         4\n",
      "        giron-d       0.87      0.75      0.81        89\n",
      "     griffith-j       0.00      0.00      0.00        12\n",
      "      grigsby-m       0.65      0.76      0.70        87\n",
      "     haedicke-m       1.00      0.29      0.44        35\n",
      "     hayslett-r       0.58      0.86      0.69       159\n",
      "        heard-m       0.81      0.91      0.86       235\n",
      "  hendrickson-s       0.00      0.00      0.00        18\n",
      "    hernandez-j       1.00      0.21      0.35        19\n",
      "        hodge-j       1.00      0.07      0.13        29\n",
      "        holst-k       0.00      0.00      0.00        11\n",
      "       horton-s       0.96      0.59      0.73        46\n",
      "        hyatt-k       0.84      0.70      0.76        96\n",
      "        jones-t       0.88      0.70      0.78        94\n",
      "     kaminski-v       0.76      0.97      0.85       497\n",
      "         kean-s       0.71      0.66      0.68       137\n",
      "       keavey-p       1.00      0.11      0.20         9\n",
      "       keiser-k       0.86      0.83      0.84       109\n",
      "         king-j       0.00      0.00      0.00         4\n",
      "      kitchen-l       0.78      0.81      0.79       339\n",
      "   kuykendall-t       0.95      0.50      0.66        38\n",
      "     lavorato-j       0.52      0.66      0.58       201\n",
      "          lay-k       0.00      0.00      0.00         4\n",
      "      lenhart-m       0.63      0.86      0.73       230\n",
      "        lewis-a       0.00      0.00      0.00         7\n",
      "        lokay-m       0.86      0.42      0.57        45\n",
      "        lokey-t       1.00      0.44      0.62        36\n",
      "         love-p       0.82      0.87      0.84       178\n",
      "        lucci-p       0.70      0.63      0.66        70\n",
      "        maggi-m       0.96      0.66      0.78        35\n",
      "         mann-k       0.85      0.59      0.70        79\n",
      "       martin-t       0.78      0.75      0.77        68\n",
      "          may-l       0.00      0.00      0.00        13\n",
      "      mccarty-d       0.96      0.46      0.62        52\n",
      "    mcconnell-m       0.74      0.52      0.61        33\n",
      "        mckay-b       1.00      0.23      0.38        13\n",
      "        mckay-j       0.85      0.72      0.78        89\n",
      "   mclaughlin-e       0.92      0.72      0.81        32\n",
      "       meyers-a       0.00      0.00      0.00         3\n",
      "mims-thurston-p       1.00      0.57      0.72        69\n",
      "       motley-m       0.00      0.00      0.00         4\n",
      "         neal-s       0.70      0.64      0.67        67\n",
      "        nemec-g       0.71      0.85      0.78       179\n",
      "        panus-s       1.00      0.12      0.22         8\n",
      "        parks-j       0.81      0.74      0.77       166\n",
      "      pereira-s       1.00      0.32      0.48        22\n",
      "  perlingiere-d       0.98      0.94      0.96       174\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       1.00      0.33      0.50        21\n",
      "      platter-p       1.00      0.28      0.44        32\n",
      "       presto-k       0.62      0.77      0.69       287\n",
      "       quenet-j       0.00      0.00      0.00         2\n",
      "      quigley-d       0.93      0.79      0.86       150\n",
      "         rapp-b       1.00      0.29      0.45        31\n",
      "    reitmeyer-j       0.78      0.44      0.56        16\n",
      "       richey-c       0.92      0.80      0.86        74\n",
      "         ring-a       0.00      0.00      0.00         8\n",
      "         ring-r       1.00      0.07      0.12        15\n",
      "       rogers-b       0.89      0.59      0.71        27\n",
      "     ruscitti-k       1.00      0.18      0.31        22\n",
      "        sager-e       0.90      0.76      0.83       125\n",
      "        saibi-e       0.00      0.00      0.00        10\n",
      "    salisbury-h       0.94      0.39      0.55        41\n",
      "      sanchez-m       0.93      0.48      0.64        29\n",
      "      sanders-r       0.79      0.78      0.78       121\n",
      "     scholtes-d       0.80      0.32      0.46        37\n",
      "  schoolcraft-d       0.89      0.89      0.89       144\n",
      "    schwieger-j       0.82      0.71      0.76        45\n",
      "        scott-s       0.57      0.83      0.67       201\n",
      "    semperger-c       0.81      0.54      0.65        79\n",
      "   shackleton-s       0.92      0.90      0.91       190\n",
      "     shankman-j       1.00      0.22      0.36        18\n",
      "      shively-h       0.73      0.29      0.42        38\n",
      "     skilling-j       1.00      0.12      0.22        16\n",
      "      slinger-r       0.50      0.07      0.12        14\n",
      "        smith-m       0.92      0.77      0.84        70\n",
      "      solberg-g       1.00      0.12      0.22        16\n",
      "        staab-t       0.79      0.79      0.79        29\n",
      "      steffes-j       0.74      0.91      0.81       414\n",
      " stepenovitch-j       1.00      0.65      0.79        23\n",
      "       storey-g       1.00      0.54      0.70        41\n",
      "        sturm-f       0.91      0.59      0.72        49\n",
      "     swerzbin-m       1.00      0.19      0.32        21\n",
      "       taylor-m       0.77      0.74      0.76       164\n",
      "        tholt-j       0.90      0.64      0.75        74\n",
      "       thomas-p       1.00      0.43      0.61        46\n",
      "     townsend-j       0.00      0.00      0.00         5\n",
      "     tycholiz-b       0.82      0.75      0.78       106\n",
      "         ward-k       0.83      0.80      0.81       203\n",
      "       watson-k       0.79      0.88      0.83       290\n",
      "       weldon-c       0.97      0.70      0.82        54\n",
      "      whalley-g       0.67      0.10      0.17        20\n",
      "        white-s       0.91      0.91      0.91       134\n",
      "        whitt-m       0.85      0.57      0.68        91\n",
      "     williams-j       0.88      0.37      0.52        38\n",
      "    williams-w3       0.65      0.88      0.75       155\n",
      "        wolfe-j       0.95      0.68      0.79        28\n",
      "       ybarbo-p       1.00      0.51      0.68        35\n",
      "       zipper-a       0.40      0.51      0.45       103\n",
      "     zufferli-j       0.77      0.62      0.69       101\n",
      "\n",
      "       accuracy                           0.75     11299\n",
      "      macro avg       0.72      0.50      0.55     11299\n",
      "   weighted avg       0.78      0.75      0.74     11299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/jtorresmolina/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/jtorresmolina/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/jtorresmolina/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models= {\"Logistic Regression 100000000\": LogisticRegression(max_iter=100)}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    #print(\"Confusion matrix:\")\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a17db4a8-a791-4733-85dd-6c6001da35c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.0591, Train Acc: 0.0054, Test Acc: 0.0032\n",
      "Epoch 2, Loss: 4.3453, Train Acc: 0.1281, Test Acc: 0.0026\n",
      "Epoch 3, Loss: 3.9598, Train Acc: 0.3263, Test Acc: 0.0026\n",
      "Epoch 4, Loss: 3.6966, Train Acc: 0.4750, Test Acc: 0.0026\n",
      "Epoch 5, Loss: 3.4932, Train Acc: 0.5640, Test Acc: 0.0026\n",
      "Epoch 6, Loss: 3.3254, Train Acc: 0.6195, Test Acc: 0.0026\n",
      "Epoch 7, Loss: 3.1817, Train Acc: 0.6545, Test Acc: 0.0035\n",
      "Epoch 8, Loss: 3.0500, Train Acc: 0.6847, Test Acc: 0.0199\n",
      "Epoch 9, Loss: 2.9318, Train Acc: 0.7029, Test Acc: 0.0206\n",
      "Epoch 10, Loss: 2.8243, Train Acc: 0.7157, Test Acc: 0.0196\n",
      "Epoch 11, Loss: 2.7227, Train Acc: 0.7289, Test Acc: 0.0232\n",
      "Epoch 12, Loss: 2.6241, Train Acc: 0.7395, Test Acc: 0.0250\n",
      "Epoch 13, Loss: 2.5378, Train Acc: 0.7470, Test Acc: 0.0265\n",
      "Epoch 14, Loss: 2.4517, Train Acc: 0.7530, Test Acc: 0.0274\n",
      "Epoch 15, Loss: 2.3732, Train Acc: 0.7625, Test Acc: 0.0281\n",
      "Epoch 16, Loss: 2.2951, Train Acc: 0.7671, Test Acc: 0.0283\n",
      "Epoch 17, Loss: 2.2177, Train Acc: 0.7748, Test Acc: 0.0294\n",
      "Epoch 18, Loss: 2.1531, Train Acc: 0.7813, Test Acc: 0.0321\n",
      "Epoch 19, Loss: 2.0801, Train Acc: 0.7875, Test Acc: 0.0378\n",
      "Epoch 20, Loss: 2.0097, Train Acc: 0.7953, Test Acc: 0.0441\n",
      "Epoch 21, Loss: 1.9489, Train Acc: 0.7984, Test Acc: 0.0476\n",
      "Epoch 22, Loss: 1.8894, Train Acc: 0.8045, Test Acc: 0.0509\n",
      "Epoch 23, Loss: 1.8312, Train Acc: 0.8088, Test Acc: 0.0550\n",
      "Epoch 24, Loss: 1.7683, Train Acc: 0.8159, Test Acc: 0.0598\n",
      "Epoch 25, Loss: 1.7181, Train Acc: 0.8184, Test Acc: 0.0671\n",
      "Epoch 26, Loss: 1.6612, Train Acc: 0.8248, Test Acc: 0.0764\n",
      "Epoch 27, Loss: 1.6177, Train Acc: 0.8297, Test Acc: 0.0862\n",
      "Epoch 28, Loss: 1.5626, Train Acc: 0.8322, Test Acc: 0.0959\n",
      "Epoch 29, Loss: 1.5158, Train Acc: 0.8374, Test Acc: 0.1037\n",
      "Epoch 30, Loss: 1.4704, Train Acc: 0.8423, Test Acc: 0.1148\n",
      "Epoch 31, Loss: 1.4220, Train Acc: 0.8455, Test Acc: 0.1321\n",
      "Epoch 32, Loss: 1.3797, Train Acc: 0.8511, Test Acc: 0.1549\n",
      "Epoch 33, Loss: 1.3390, Train Acc: 0.8552, Test Acc: 0.1814\n",
      "Epoch 34, Loss: 1.2994, Train Acc: 0.8584, Test Acc: 0.2084\n",
      "Epoch 35, Loss: 1.2535, Train Acc: 0.8638, Test Acc: 0.2351\n",
      "Epoch 36, Loss: 1.2136, Train Acc: 0.8668, Test Acc: 0.2629\n",
      "Epoch 37, Loss: 1.1877, Train Acc: 0.8697, Test Acc: 0.2931\n",
      "Epoch 38, Loss: 1.1478, Train Acc: 0.8757, Test Acc: 0.3208\n",
      "Epoch 39, Loss: 1.1163, Train Acc: 0.8771, Test Acc: 0.3454\n",
      "Epoch 40, Loss: 1.0783, Train Acc: 0.8794, Test Acc: 0.3695\n",
      "Epoch 41, Loss: 1.0452, Train Acc: 0.8838, Test Acc: 0.3878\n",
      "Epoch 42, Loss: 1.0182, Train Acc: 0.8864, Test Acc: 0.4045\n",
      "Epoch 43, Loss: 0.9854, Train Acc: 0.8902, Test Acc: 0.4174\n",
      "Epoch 44, Loss: 0.9567, Train Acc: 0.8944, Test Acc: 0.4317\n",
      "Epoch 45, Loss: 0.9318, Train Acc: 0.8958, Test Acc: 0.4475\n",
      "Epoch 46, Loss: 0.9088, Train Acc: 0.8981, Test Acc: 0.4624\n",
      "Epoch 47, Loss: 0.8745, Train Acc: 0.9018, Test Acc: 0.4777\n",
      "Epoch 48, Loss: 0.8531, Train Acc: 0.9045, Test Acc: 0.4955\n",
      "Epoch 49, Loss: 0.8284, Train Acc: 0.9088, Test Acc: 0.5128\n",
      "Epoch 50, Loss: 0.8058, Train Acc: 0.9103, Test Acc: 0.5296\n",
      "Epoch 51, Loss: 0.7818, Train Acc: 0.9132, Test Acc: 0.5456\n",
      "Epoch 52, Loss: 0.7565, Train Acc: 0.9156, Test Acc: 0.5631\n",
      "Epoch 53, Loss: 0.7416, Train Acc: 0.9174, Test Acc: 0.5790\n",
      "Epoch 54, Loss: 0.7181, Train Acc: 0.9201, Test Acc: 0.5928\n",
      "Epoch 55, Loss: 0.6963, Train Acc: 0.9230, Test Acc: 0.6088\n",
      "Epoch 56, Loss: 0.6732, Train Acc: 0.9256, Test Acc: 0.6232\n",
      "Epoch 57, Loss: 0.6644, Train Acc: 0.9266, Test Acc: 0.6353\n",
      "Epoch 58, Loss: 0.6401, Train Acc: 0.9311, Test Acc: 0.6464\n",
      "Epoch 59, Loss: 0.6213, Train Acc: 0.9321, Test Acc: 0.6590\n",
      "Epoch 60, Loss: 0.6067, Train Acc: 0.9332, Test Acc: 0.6674\n",
      "Epoch 61, Loss: 0.5914, Train Acc: 0.9337, Test Acc: 0.6778\n",
      "Epoch 62, Loss: 0.5786, Train Acc: 0.9367, Test Acc: 0.6862\n",
      "Epoch 63, Loss: 0.5598, Train Acc: 0.9378, Test Acc: 0.6943\n",
      "Epoch 64, Loss: 0.5456, Train Acc: 0.9405, Test Acc: 0.6999\n",
      "Epoch 65, Loss: 0.5322, Train Acc: 0.9404, Test Acc: 0.7043\n",
      "Epoch 66, Loss: 0.5166, Train Acc: 0.9442, Test Acc: 0.7099\n",
      "Epoch 67, Loss: 0.5074, Train Acc: 0.9430, Test Acc: 0.7147\n",
      "Epoch 68, Loss: 0.4913, Train Acc: 0.9456, Test Acc: 0.7201\n",
      "Epoch 69, Loss: 0.4767, Train Acc: 0.9483, Test Acc: 0.7233\n",
      "Epoch 70, Loss: 0.4633, Train Acc: 0.9500, Test Acc: 0.7271\n",
      "Epoch 71, Loss: 0.4548, Train Acc: 0.9507, Test Acc: 0.7300\n",
      "Epoch 72, Loss: 0.4458, Train Acc: 0.9516, Test Acc: 0.7313\n",
      "Epoch 73, Loss: 0.4318, Train Acc: 0.9534, Test Acc: 0.7348\n",
      "Epoch 74, Loss: 0.4223, Train Acc: 0.9537, Test Acc: 0.7363\n",
      "Epoch 75, Loss: 0.4126, Train Acc: 0.9553, Test Acc: 0.7377\n",
      "Epoch 76, Loss: 0.4058, Train Acc: 0.9573, Test Acc: 0.7391\n",
      "Epoch 77, Loss: 0.3927, Train Acc: 0.9585, Test Acc: 0.7400\n",
      "Epoch 78, Loss: 0.3837, Train Acc: 0.9595, Test Acc: 0.7412\n",
      "Epoch 79, Loss: 0.3768, Train Acc: 0.9593, Test Acc: 0.7422\n",
      "Epoch 80, Loss: 0.3673, Train Acc: 0.9617, Test Acc: 0.7430\n",
      "Epoch 81, Loss: 0.3602, Train Acc: 0.9610, Test Acc: 0.7436\n",
      "Epoch 82, Loss: 0.3500, Train Acc: 0.9634, Test Acc: 0.7436\n",
      "Epoch 83, Loss: 0.3444, Train Acc: 0.9650, Test Acc: 0.7432\n",
      "Epoch 84, Loss: 0.3371, Train Acc: 0.9653, Test Acc: 0.7438\n",
      "Epoch 85, Loss: 0.3273, Train Acc: 0.9657, Test Acc: 0.7433\n",
      "Epoch 86, Loss: 0.3217, Train Acc: 0.9665, Test Acc: 0.7442\n",
      "Epoch 87, Loss: 0.3164, Train Acc: 0.9675, Test Acc: 0.7454\n",
      "Epoch 88, Loss: 0.3096, Train Acc: 0.9685, Test Acc: 0.7457\n",
      "Epoch 89, Loss: 0.2975, Train Acc: 0.9698, Test Acc: 0.7453\n",
      "Epoch 90, Loss: 0.2932, Train Acc: 0.9699, Test Acc: 0.7459\n",
      "Epoch 91, Loss: 0.2886, Train Acc: 0.9711, Test Acc: 0.7460\n",
      "Epoch 92, Loss: 0.2834, Train Acc: 0.9724, Test Acc: 0.7456\n",
      "Epoch 93, Loss: 0.2760, Train Acc: 0.9722, Test Acc: 0.7448\n",
      "Epoch 94, Loss: 0.2696, Train Acc: 0.9736, Test Acc: 0.7450\n",
      "Epoch 95, Loss: 0.2676, Train Acc: 0.9734, Test Acc: 0.7444\n",
      "Epoch 96, Loss: 0.2625, Train Acc: 0.9739, Test Acc: 0.7444\n",
      "Epoch 97, Loss: 0.2565, Train Acc: 0.9752, Test Acc: 0.7449\n",
      "Epoch 98, Loss: 0.2522, Train Acc: 0.9752, Test Acc: 0.7458\n",
      "Epoch 99, Loss: 0.2477, Train Acc: 0.9755, Test Acc: 0.7464\n",
      "Epoch 100, Loss: 0.2435, Train Acc: 0.9761, Test Acc: 0.7463\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels_filt)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.output = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.output(x)\n",
    "\"\"\"\n",
    "input_dim=X_train_tensor.shape[1]\n",
    "model = Net(input_dim=input_dim, hidden_dim1=256,hidden_dim2=128, output_dim=len(le.classes_))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "num_epochs =30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.output = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
    "        return self.output(x)\n",
    "\n",
    "# Use correct input_dim\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = DeeperNet(input_dim=input_dim, hidden_dim1=256, hidden_dim2=128, output_dim=len(le.classes_))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\"\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1, hidden2, hidden3, output_dim):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.output = nn.Linear(hidden3, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(self.relu(self.bn3(self.fc3(x))))\n",
    "        return self.output(x)\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = len(le.classes_)\n",
    "\n",
    "model = DeeperNet(input_dim=input_dim, hidden1=512, hidden2=256, hidden3=128, output_dim=output_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "num_epochs=100\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track training loss and accuracy\n",
    "    train_losses.append(loss.item())\n",
    "    _, predicted_train = torch.max(outputs, 1)\n",
    "    correct_train = (predicted_train == y_train_tensor).sum().item()\n",
    "    train_acc = correct_train / len(y_train_tensor)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        _, predicted_test = torch.max(outputs_test, 1)\n",
    "        correct_test = (predicted_test == y_test_tensor).sum().item()\n",
    "        test_acc = correct_test / len(y_test_tensor)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62453ae9-4521-4ea3-934e-f725b185da43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, marker='o', label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, marker='s', label=\"Test Accuracy\")\n",
    "plt.title(\"Training vs. Test Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74444de7-194a-486b-b3be-a927a2f522c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7463\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.81      0.73      0.77       103\n",
      "       arnold-j       0.66      0.70      0.68       216\n",
      "        arora-h       0.62      0.25      0.36        20\n",
      "       badeer-r       0.00      0.00      0.00         2\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.76      0.62      0.68        73\n",
      "     baughman-d       0.73      0.57      0.64        28\n",
      "         beck-s       0.75      0.81      0.77       145\n",
      "       benson-r       0.00      0.00      0.00         5\n",
      "        blair-l       0.83      0.90      0.86       278\n",
      "      brawner-s       0.73      0.36      0.48        22\n",
      "          buy-r       0.79      0.81      0.80       117\n",
      "     campbell-l       0.82      0.68      0.74        34\n",
      "       carson-m       0.93      0.74      0.82        34\n",
      "         cash-m       0.86      0.88      0.87       164\n",
      "    causholli-m       0.95      0.94      0.95        67\n",
      "       corman-s       0.68      0.63      0.65       188\n",
      "     crandell-s       0.90      0.61      0.73        44\n",
      "       cuilla-m       0.77      0.66      0.71        41\n",
      "     dasovich-j       0.79      0.87      0.83       430\n",
      "        davis-d       0.88      0.50      0.64        14\n",
      "         dean-c       1.00      0.20      0.33        10\n",
      "     delainey-d       0.86      0.33      0.48        18\n",
      "      derrick-j       0.89      0.75      0.81       175\n",
      "       donoho-l       0.63      0.60      0.61        65\n",
      "      donohoe-t       0.80      0.44      0.57         9\n",
      "      dorland-c       0.79      0.84      0.81       176\n",
      "        ermis-f       0.00      0.00      0.00         5\n",
      "       farmer-d       0.71      0.74      0.73        77\n",
      "      fischer-m       1.00      0.58      0.73        19\n",
      "       forney-j       0.92      0.90      0.91       128\n",
      "         gang-l       0.92      0.89      0.91        27\n",
      "          gay-r       0.00      0.00      0.00         3\n",
      "     geaccone-t       0.76      0.67      0.71       109\n",
      "      germany-c       0.75      0.80      0.77       401\n",
      " gilbertsmith-d       0.00      0.00      0.00         4\n",
      "        giron-d       0.81      0.75      0.78        89\n",
      "     griffith-j       0.83      0.42      0.56        12\n",
      "      grigsby-m       0.60      0.64      0.62        87\n",
      "     haedicke-m       0.52      0.46      0.48        35\n",
      "     hayslett-r       0.64      0.69      0.66       159\n",
      "        heard-m       0.80      0.89      0.84       235\n",
      "  hendrickson-s       0.58      0.39      0.47        18\n",
      "    hernandez-j       0.92      0.58      0.71        19\n",
      "        hodge-j       0.37      0.24      0.29        29\n",
      "        holst-k       0.50      0.18      0.27        11\n",
      "       horton-s       0.71      0.70      0.70        46\n",
      "        hyatt-k       0.88      0.76      0.82        96\n",
      "        jones-t       0.91      0.74      0.82        94\n",
      "     kaminski-v       0.90      0.90      0.90       497\n",
      "         kean-s       0.66      0.53      0.59       137\n",
      "       keavey-p       0.80      0.44      0.57         9\n",
      "       keiser-k       0.81      0.83      0.82       109\n",
      "         king-j       0.00      0.00      0.00         4\n",
      "      kitchen-l       0.67      0.75      0.71       339\n",
      "   kuykendall-t       0.66      0.76      0.71        38\n",
      "     lavorato-j       0.47      0.61      0.53       201\n",
      "          lay-k       0.00      0.00      0.00         4\n",
      "      lenhart-m       0.68      0.76      0.72       230\n",
      "        lewis-a       0.00      0.00      0.00         7\n",
      "        lokay-m       0.67      0.64      0.66        45\n",
      "        lokey-t       0.77      0.64      0.70        36\n",
      "         love-p       0.78      0.87      0.82       178\n",
      "        lucci-p       0.65      0.71      0.68        70\n",
      "        maggi-m       0.60      0.77      0.68        35\n",
      "         mann-k       0.65      0.67      0.66        79\n",
      "       martin-t       0.71      0.62      0.66        68\n",
      "          may-l       0.44      0.31      0.36        13\n",
      "      mccarty-d       0.67      0.63      0.65        52\n",
      "    mcconnell-m       0.79      0.70      0.74        33\n",
      "        mckay-b       1.00      0.77      0.87        13\n",
      "        mckay-j       0.85      0.76      0.80        89\n",
      "   mclaughlin-e       0.71      0.69      0.70        32\n",
      "       meyers-a       0.00      0.00      0.00         3\n",
      "mims-thurston-p       0.85      0.72      0.78        69\n",
      "       motley-m       0.00      0.00      0.00         4\n",
      "         neal-s       0.59      0.69      0.63        67\n",
      "        nemec-g       0.75      0.80      0.78       179\n",
      "        panus-s       1.00      0.38      0.55         8\n",
      "        parks-j       0.75      0.78      0.77       166\n",
      "      pereira-s       0.84      0.73      0.78        22\n",
      "  perlingiere-d       0.85      0.90      0.88       174\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       0.67      0.57      0.62        21\n",
      "      platter-p       0.81      0.41      0.54        32\n",
      "       presto-k       0.61      0.71      0.66       287\n",
      "       quenet-j       0.00      0.00      0.00         2\n",
      "      quigley-d       0.76      0.83      0.79       150\n",
      "         rapp-b       0.64      0.45      0.53        31\n",
      "    reitmeyer-j       0.69      0.56      0.62        16\n",
      "       richey-c       0.88      0.80      0.84        74\n",
      "         ring-a       0.00      0.00      0.00         8\n",
      "         ring-r       0.89      0.53      0.67        15\n",
      "       rogers-b       0.87      0.74      0.80        27\n",
      "     ruscitti-k       0.72      0.59      0.65        22\n",
      "        sager-e       0.81      0.70      0.76       125\n",
      "        saibi-e       1.00      0.30      0.46        10\n",
      "    salisbury-h       0.78      0.61      0.68        41\n",
      "      sanchez-m       0.76      0.55      0.64        29\n",
      "      sanders-r       0.79      0.72      0.75       121\n",
      "     scholtes-d       0.63      0.65      0.64        37\n",
      "  schoolcraft-d       0.85      0.89      0.87       144\n",
      "    schwieger-j       0.77      0.67      0.71        45\n",
      "        scott-s       0.71      0.78      0.75       201\n",
      "    semperger-c       0.60      0.58      0.59        79\n",
      "   shackleton-s       0.83      0.92      0.87       190\n",
      "     shankman-j       0.79      0.61      0.69        18\n",
      "      shively-h       0.48      0.42      0.45        38\n",
      "     skilling-j       0.91      0.62      0.74        16\n",
      "      slinger-r       0.60      0.21      0.32        14\n",
      "        smith-m       0.86      0.86      0.86        70\n",
      "      solberg-g       0.86      0.38      0.52        16\n",
      "        staab-t       0.83      0.83      0.83        29\n",
      "      steffes-j       0.78      0.85      0.81       414\n",
      " stepenovitch-j       0.85      0.74      0.79        23\n",
      "       storey-g       0.76      0.63      0.69        41\n",
      "        sturm-f       0.65      0.71      0.68        49\n",
      "     swerzbin-m       0.59      0.48      0.53        21\n",
      "       taylor-m       0.66      0.71      0.69       164\n",
      "        tholt-j       0.74      0.74      0.74        74\n",
      "       thomas-p       0.78      0.61      0.68        46\n",
      "     townsend-j       0.00      0.00      0.00         5\n",
      "     tycholiz-b       0.72      0.67      0.69       106\n",
      "         ward-k       0.71      0.77      0.74       203\n",
      "       watson-k       0.79      0.82      0.80       290\n",
      "       weldon-c       0.70      0.57      0.63        54\n",
      "      whalley-g       0.80      0.20      0.32        20\n",
      "        white-s       0.83      0.84      0.84       134\n",
      "        whitt-m       0.66      0.63      0.64        91\n",
      "     williams-j       0.77      0.45      0.57        38\n",
      "    williams-w3       0.66      0.81      0.73       155\n",
      "        wolfe-j       0.83      0.86      0.84        28\n",
      "       ybarbo-p       0.70      0.60      0.65        35\n",
      "       zipper-a       0.37      0.40      0.38       103\n",
      "     zufferli-j       0.68      0.67      0.68       101\n",
      "\n",
      "       accuracy                           0.75     11299\n",
      "      macro avg       0.67      0.58      0.61     11299\n",
      "   weighted avg       0.75      0.75      0.74     11299\n",
      "\n",
      "F1 Score (Weighted): 0.7419\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 75   2   0 ...   0   0   1]\n",
      " [  0 151   0 ...   0   2   1]\n",
      " [  0   0   5 ...   0   2   0]\n",
      " ...\n",
      " [  0   0   0 ...  21   0   0]\n",
      " [  0   5   0 ...   0  41   0]\n",
      " [  1   0   0 ...   0   1  68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(y_pred_logits, dim=1)\n",
    "\n",
    "# Convert predictions and true labels to numpy for sklearn\n",
    "y_pred_np = y_pred.numpy()\n",
    "y_test_np = y_test_tensor.numpy()\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test_np, y_pred_np):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_np, y_pred_np, target_names=le.classes_, zero_division=0))\n",
    "# Weighted F1 Score: accounts for class imbalance\n",
    "f1_weighted = f1_score(y_test_np, y_pred_np, average='weighted', zero_division=0)\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_np, y_pred_np))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07312423-367d-43fb-935d-d768ed489181",
   "metadata": {},
   "source": [
    "100 epochs: 0.723\n",
    "125 epochs: 0.71\n",
    "80 epochs:0.7340\n",
    "50 epochs: 0.5918\n",
    "90 epochs: 0.7357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04103797-a0df-4f66-8d19-15344a98e53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "140wi25",
   "language": "python",
   "name": "140wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
