{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1e9bcd-57d4-4831-9d38-bc93800a7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)      # remove ursls\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)               # and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # add stemming or lemmatizngn??\n",
    "    return text\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d16ed634-e7b4-49b3-8e22-dfa97cfa64b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def load_emails(maildir: str):\n",
    "    texts, labels = [], []\n",
    "    cnt=0\n",
    "    for user in os.listdir(maildir):\n",
    "        user_dir = os.path.join(maildir, user)\n",
    "        if not os.path.isdir(user_dir):\n",
    "            continue\n",
    "\n",
    "        # only look at the “sent_items” folder\n",
    "        folder_dir = os.path.join(user_dir, \"sent_items\")\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        if not os.path.isdir(folder_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder_dir):\n",
    "            path = os.path.join(folder_dir, fname)\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    msg = BytesParser(policy=policy.default).parse(f) #read binary\n",
    "                body = msg.get_body(preferencelist=('plain',))\n",
    "                if body is None:\n",
    "                    continue\n",
    "                raw = body.get_content()\n",
    "                text = clean_text(raw)\n",
    "                if not text:\n",
    "                    continue\n",
    "                tokens = [w for w in text.split() if w not in STOPWORDS]  #cleanning. Added lemmentatizn or stemming?\n",
    "                texts.append(' '.join(tokens))\n",
    "                labels.append(user)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return texts, labels\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a84a34a5-5b42-433a-8f79-72651a3f50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning emails…\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "→ 37689 messages from 136 authors\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../maildir\"     # path to Enron stuf\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Loading and cleaning emails…\")\n",
    "texts, labels = load_emails(DATA_DIR)\n",
    "print(f\"→ {len(texts)} messages from {len(set(labels))} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018d0c61-5d9c-4d6e-a4b6-4d245ca9acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing with TF–IDF…\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing with TF–IDF…\")\n",
    "vect = TfidfVectorizer(max_features=20_000)\n",
    "X = vect.fit_transform(texts)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "914d64f1-689b-453d-9e63-793ca6a13686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on 30150 docs; testing on 7538\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(labels)\n",
    "\n",
    "keep = {lbl for lbl, cnt in counts.items() if cnt >= 2}\n",
    "\n",
    "texts_filt = [t for t, l in zip(texts, labels) if l in keep]\n",
    "labels_filt = [l for l in labels if l in keep]\n",
    "\n",
    "X = vect.transform(texts_filt)   \n",
    "y = labels_filt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #split and train\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"→ Training on {X_train.shape[0]} docs; testing on {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4952d2c-8208-4fb4-8595-b6c45c760545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression 4000\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    #\"Naive Bayes\": MultinomialNB(alpha=1.0),\n",
    "    \"Logistic Regression 4000\": LogisticRegression(max_iter=4000),\n",
    "    \"Logistic Regression 2000\": LogisticRegression(max_iter=2000),\n",
    "    \"Logistic Regression 1000\": LogisticRegression(max_iter=1000),\n",
    "    \"Logistic Regression 500\": LogisticRegression(max_iter=500),\n",
    "    \"Logistic Regression 250\": LogisticRegression(max_iter=250),\n",
    "    \"Logistic Regression 125\": LogisticRegression(max_iter=125),\n",
    "    \"Logistic Regression 62\": LogisticRegression(max_iter=62),\n",
    "    \"Logistic Regression 31\": LogisticRegression(max_iter=31),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(max_depth=20)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    #print(\"Confusion matrix:\")\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c359b-7bc3-430d-a189-21fbcb7b6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistical regression is the best. Preform For the full data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "140wi25",
   "language": "python",
   "name": "140wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
