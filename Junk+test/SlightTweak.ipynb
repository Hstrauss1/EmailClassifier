{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1e9bcd-57d4-4831-9d38-bc93800a7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # strip URLs\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)           # strip punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # collapse whitespace\n",
    "\n",
    "    tokens = [\n",
    "        stemmer.stem(word)\n",
    "        for word in text.split()\n",
    "        if word not in STOPWORDS\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16ed634-e7b4-49b3-8e22-dfa97cfa64b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def load_emails(maildir: str):\n",
    "    texts, labels = [], []\n",
    "    cnt=0\n",
    "    for user in os.listdir(maildir):\n",
    "        user_dir = os.path.join(maildir, user)\n",
    "        if not os.path.isdir(user_dir):\n",
    "            continue\n",
    "\n",
    "        # only look at the “sent_items” folder\n",
    "        folder_dir = os.path.join(user_dir, \"sent_items\")\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        if not os.path.isdir(folder_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder_dir):\n",
    "            path = os.path.join(folder_dir, fname)\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    msg = BytesParser(policy=policy.default).parse(f) #read binary\n",
    "                body = msg.get_body(preferencelist=('plain',))\n",
    "                if body is None:\n",
    "                    continue\n",
    "                raw = body.get_content()\n",
    "                text = clean_text(raw)\n",
    "                if not text:\n",
    "                    continue\n",
    "                tokens = [w for w in text.split() if w not in STOPWORDS]  #cleanning. Added lemmentatizn or stemming?\n",
    "                texts.append(' '.join(tokens))\n",
    "                labels.append(user)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return texts, labels\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a34a5-5b42-433a-8f79-72651a3f50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning emails…\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/WAVE/projects/CSEN-140-Sp25/HHJ140Proj/Sent_Items_only\"     # path to Enron stuf\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 36\n",
    "\n",
    "print(\"Loading and cleaning emails…\")\n",
    "texts, labels = load_emails(DATA_DIR)\n",
    "print(f\"→ {len(texts)} messages from {len(set(labels))} authors\")\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018d0c61-5d9c-4d6e-a4b6-4d245ca9acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing with TF–IDF…\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing with TF–IDF…\")\n",
    "vect = TfidfVectorizer(max_features=20_000)\n",
    "X = vect.fit_transform(texts)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914d64f1-689b-453d-9e63-793ca6a13686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on 26364 docs; testing on 11299\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(labels)\n",
    "\n",
    "keep = {lbl for lbl, cnt in counts.items() if cnt >= 2}\n",
    "\n",
    "texts_filt = [t for t, l in zip(texts, labels) if l in keep]\n",
    "labels_filt = [l for l in labels if l in keep]\n",
    "\n",
    "X = vect.transform(texts_filt)   \n",
    "y = labels_filt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #split and train\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"→ Training on {X_train.shape[0]} docs; testing on {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4952d2c-8208-4fb4-8595-b6c45c760545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression 125\n",
      "fitted\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.90      0.68      0.77       103\n",
      "       arnold-j       0.69      0.69      0.69       216\n",
      "        arora-h       1.00      0.30      0.46        20\n",
      "       badeer-r       0.00      0.00      0.00         2\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.79      0.66      0.72        73\n",
      "     baughman-d       0.93      0.46      0.62        28\n",
      "         beck-s       0.84      0.90      0.87       145\n",
      "       benson-r       0.00      0.00      0.00         5\n",
      "        blair-l       0.90      0.96      0.93       278\n",
      "      brawner-s       0.83      0.23      0.36        22\n",
      "          buy-r       0.91      0.82      0.86       117\n",
      "     campbell-l       0.81      0.50      0.62        34\n",
      "       carson-m       0.96      0.68      0.79        34\n",
      "         cash-m       0.87      0.87      0.87       164\n",
      "    causholli-m       0.98      0.94      0.96        67\n",
      "       corman-s       0.78      0.70      0.73       188\n",
      "     crandell-s       0.90      0.61      0.73        44\n",
      "       cuilla-m       1.00      0.44      0.61        41\n",
      "     dasovich-j       0.74      0.92      0.82       430\n",
      "        davis-d       1.00      0.43      0.60        14\n",
      "         dean-c       0.00      0.00      0.00        10\n",
      "     delainey-d       1.00      0.06      0.11        18\n",
      "      derrick-j       0.78      0.91      0.84       175\n",
      "       donoho-l       0.89      0.60      0.72        65\n",
      "      donohoe-t       0.00      0.00      0.00         9\n",
      "      dorland-c       0.77      0.86      0.81       176\n",
      "        ermis-f       0.00      0.00      0.00         5\n",
      "       farmer-d       0.94      0.64      0.76        77\n",
      "      fischer-m       1.00      0.32      0.48        19\n",
      "       forney-j       0.97      0.95      0.96       128\n",
      "         gang-l       1.00      0.70      0.83        27\n",
      "          gay-r       0.00      0.00      0.00         3\n",
      "     geaccone-t       0.85      0.69      0.76       109\n",
      "      germany-c       0.43      0.91      0.59       401\n",
      " gilbertsmith-d       0.00      0.00      0.00         4\n",
      "        giron-d       0.87      0.75      0.81        89\n",
      "     griffith-j       0.00      0.00      0.00        12\n",
      "      grigsby-m       0.65      0.76      0.70        87\n",
      "     haedicke-m       1.00      0.29      0.44        35\n",
      "     hayslett-r       0.58      0.86      0.69       159\n",
      "        heard-m       0.81      0.91      0.86       235\n",
      "  hendrickson-s       0.00      0.00      0.00        18\n",
      "    hernandez-j       1.00      0.21      0.35        19\n",
      "        hodge-j       1.00      0.07      0.13        29\n",
      "        holst-k       0.00      0.00      0.00        11\n",
      "       horton-s       0.96      0.59      0.73        46\n",
      "        hyatt-k       0.84      0.70      0.76        96\n",
      "        jones-t       0.88      0.70      0.78        94\n",
      "     kaminski-v       0.76      0.97      0.85       497\n",
      "         kean-s       0.71      0.66      0.68       137\n",
      "       keavey-p       1.00      0.11      0.20         9\n",
      "       keiser-k       0.86      0.83      0.84       109\n",
      "         king-j       0.00      0.00      0.00         4\n",
      "      kitchen-l       0.78      0.81      0.79       339\n",
      "   kuykendall-t       0.95      0.50      0.66        38\n",
      "     lavorato-j       0.52      0.66      0.58       201\n",
      "          lay-k       0.00      0.00      0.00         4\n",
      "      lenhart-m       0.63      0.86      0.73       230\n",
      "        lewis-a       0.00      0.00      0.00         7\n",
      "        lokay-m       0.86      0.42      0.57        45\n",
      "        lokey-t       1.00      0.44      0.62        36\n",
      "         love-p       0.82      0.87      0.84       178\n",
      "        lucci-p       0.70      0.63      0.66        70\n",
      "        maggi-m       0.96      0.66      0.78        35\n",
      "         mann-k       0.85      0.59      0.70        79\n",
      "       martin-t       0.78      0.75      0.77        68\n",
      "          may-l       0.00      0.00      0.00        13\n",
      "      mccarty-d       0.96      0.46      0.62        52\n",
      "    mcconnell-m       0.74      0.52      0.61        33\n",
      "        mckay-b       1.00      0.23      0.38        13\n",
      "        mckay-j       0.85      0.72      0.78        89\n",
      "   mclaughlin-e       0.92      0.72      0.81        32\n",
      "       meyers-a       0.00      0.00      0.00         3\n",
      "mims-thurston-p       1.00      0.57      0.72        69\n",
      "       motley-m       0.00      0.00      0.00         4\n",
      "         neal-s       0.70      0.64      0.67        67\n",
      "        nemec-g       0.71      0.85      0.78       179\n",
      "        panus-s       1.00      0.12      0.22         8\n",
      "        parks-j       0.81      0.74      0.77       166\n",
      "      pereira-s       1.00      0.32      0.48        22\n",
      "  perlingiere-d       0.98      0.94      0.96       174\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       1.00      0.33      0.50        21\n",
      "      platter-p       1.00      0.28      0.44        32\n",
      "       presto-k       0.62      0.77      0.69       287\n",
      "       quenet-j       0.00      0.00      0.00         2\n",
      "      quigley-d       0.93      0.79      0.86       150\n",
      "         rapp-b       1.00      0.29      0.45        31\n",
      "    reitmeyer-j       0.78      0.44      0.56        16\n",
      "       richey-c       0.92      0.80      0.86        74\n",
      "         ring-a       0.00      0.00      0.00         8\n",
      "         ring-r       1.00      0.07      0.12        15\n",
      "       rogers-b       0.89      0.59      0.71        27\n",
      "     ruscitti-k       1.00      0.18      0.31        22\n",
      "        sager-e       0.90      0.76      0.83       125\n",
      "        saibi-e       0.00      0.00      0.00        10\n",
      "    salisbury-h       0.94      0.39      0.55        41\n",
      "      sanchez-m       0.93      0.48      0.64        29\n",
      "      sanders-r       0.79      0.78      0.78       121\n",
      "     scholtes-d       0.80      0.32      0.46        37\n",
      "  schoolcraft-d       0.89      0.89      0.89       144\n",
      "    schwieger-j       0.82      0.71      0.76        45\n",
      "        scott-s       0.57      0.83      0.67       201\n",
      "    semperger-c       0.81      0.54      0.65        79\n",
      "   shackleton-s       0.92      0.90      0.91       190\n",
      "     shankman-j       1.00      0.22      0.36        18\n",
      "      shively-h       0.73      0.29      0.42        38\n",
      "     skilling-j       1.00      0.12      0.22        16\n",
      "      slinger-r       0.50      0.07      0.12        14\n",
      "        smith-m       0.92      0.77      0.84        70\n",
      "      solberg-g       1.00      0.12      0.22        16\n",
      "        staab-t       0.79      0.79      0.79        29\n",
      "      steffes-j       0.74      0.91      0.81       414\n",
      " stepenovitch-j       1.00      0.65      0.79        23\n",
      "       storey-g       1.00      0.54      0.70        41\n",
      "        sturm-f       0.91      0.59      0.72        49\n",
      "     swerzbin-m       1.00      0.19      0.32        21\n",
      "       taylor-m       0.77      0.74      0.76       164\n",
      "        tholt-j       0.90      0.64      0.75        74\n",
      "       thomas-p       1.00      0.43      0.61        46\n",
      "     townsend-j       0.00      0.00      0.00         5\n",
      "     tycholiz-b       0.82      0.75      0.78       106\n",
      "         ward-k       0.83      0.80      0.81       203\n",
      "       watson-k       0.79      0.88      0.83       290\n",
      "       weldon-c       0.97      0.70      0.82        54\n",
      "      whalley-g       0.67      0.10      0.17        20\n",
      "        white-s       0.91      0.91      0.91       134\n",
      "        whitt-m       0.85      0.57      0.68        91\n",
      "     williams-j       0.88      0.37      0.52        38\n",
      "    williams-w3       0.65      0.88      0.75       155\n",
      "        wolfe-j       0.95      0.68      0.79        28\n",
      "       ybarbo-p       1.00      0.51      0.68        35\n",
      "       zipper-a       0.40      0.51      0.45       103\n",
      "     zufferli-j       0.77      0.62      0.69       101\n",
      "\n",
      "       accuracy                           0.75     11299\n",
      "      macro avg       0.72      0.50      0.55     11299\n",
      "   weighted avg       0.78      0.75      0.74     11299\n",
      "\n",
      "\n",
      "Logistic Regression 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.89      0.68      0.77       103\n",
      "       arnold-j       0.69      0.69      0.69       216\n",
      "        arora-h       1.00      0.30      0.46        20\n",
      "       badeer-r       0.00      0.00      0.00         2\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.77      0.66      0.71        73\n",
      "     baughman-d       0.93      0.46      0.62        28\n",
      "         beck-s       0.84      0.90      0.87       145\n",
      "       benson-r       0.00      0.00      0.00         5\n",
      "        blair-l       0.90      0.96      0.93       278\n",
      "      brawner-s       0.80      0.18      0.30        22\n",
      "          buy-r       0.91      0.82      0.86       117\n",
      "     campbell-l       0.81      0.50      0.62        34\n",
      "       carson-m       0.96      0.68      0.79        34\n",
      "         cash-m       0.87      0.87      0.87       164\n",
      "    causholli-m       0.98      0.94      0.96        67\n",
      "       corman-s       0.77      0.70      0.73       188\n",
      "     crandell-s       0.90      0.61      0.73        44\n",
      "       cuilla-m       1.00      0.44      0.61        41\n",
      "     dasovich-j       0.74      0.91      0.82       430\n",
      "        davis-d       1.00      0.43      0.60        14\n",
      "         dean-c       0.00      0.00      0.00        10\n",
      "     delainey-d       1.00      0.06      0.11        18\n",
      "      derrick-j       0.78      0.91      0.84       175\n",
      "       donoho-l       0.87      0.60      0.71        65\n",
      "      donohoe-t       0.00      0.00      0.00         9\n",
      "      dorland-c       0.77      0.86      0.81       176\n",
      "        ermis-f       0.00      0.00      0.00         5\n",
      "       farmer-d       0.94      0.64      0.76        77\n",
      "      fischer-m       1.00      0.32      0.48        19\n",
      "       forney-j       0.97      0.95      0.96       128\n",
      "         gang-l       1.00      0.67      0.80        27\n",
      "          gay-r       0.00      0.00      0.00         3\n",
      "     geaccone-t       0.85      0.70      0.77       109\n",
      "      germany-c       0.45      0.90      0.60       401\n",
      " gilbertsmith-d       0.00      0.00      0.00         4\n",
      "        giron-d       0.87      0.75      0.81        89\n",
      "     griffith-j       0.00      0.00      0.00        12\n",
      "      grigsby-m       0.65      0.76      0.70        87\n",
      "     haedicke-m       1.00      0.29      0.44        35\n",
      "     hayslett-r       0.58      0.87      0.70       159\n",
      "        heard-m       0.81      0.91      0.86       235\n",
      "  hendrickson-s       0.00      0.00      0.00        18\n",
      "    hernandez-j       1.00      0.21      0.35        19\n",
      "        hodge-j       1.00      0.07      0.13        29\n",
      "        holst-k       0.00      0.00      0.00        11\n",
      "       horton-s       1.00      0.59      0.74        46\n",
      "        hyatt-k       0.84      0.70      0.76        96\n",
      "        jones-t       0.88      0.70      0.78        94\n",
      "     kaminski-v       0.76      0.97      0.85       497\n",
      "         kean-s       0.71      0.66      0.68       137\n",
      "       keavey-p       1.00      0.11      0.20         9\n",
      "       keiser-k       0.86      0.83      0.84       109\n",
      "         king-j       0.00      0.00      0.00         4\n",
      "      kitchen-l       0.77      0.81      0.79       339\n",
      "   kuykendall-t       0.90      0.50      0.64        38\n",
      "     lavorato-j       0.51      0.66      0.58       201\n",
      "          lay-k       0.00      0.00      0.00         4\n",
      "      lenhart-m       0.61      0.86      0.72       230\n",
      "        lewis-a       0.00      0.00      0.00         7\n",
      "        lokay-m       0.86      0.42      0.57        45\n",
      "        lokey-t       1.00      0.47      0.64        36\n",
      "         love-p       0.82      0.87      0.84       178\n",
      "        lucci-p       0.69      0.64      0.67        70\n",
      "        maggi-m       0.96      0.63      0.76        35\n",
      "         mann-k       0.85      0.59      0.70        79\n",
      "       martin-t       0.78      0.75      0.77        68\n",
      "          may-l       0.00      0.00      0.00        13\n",
      "      mccarty-d       1.00      0.46      0.63        52\n",
      "    mcconnell-m       0.74      0.52      0.61        33\n",
      "        mckay-b       1.00      0.23      0.38        13\n",
      "        mckay-j       0.86      0.73      0.79        89\n",
      "   mclaughlin-e       0.92      0.72      0.81        32\n",
      "       meyers-a       0.00      0.00      0.00         3\n",
      "mims-thurston-p       1.00      0.57      0.72        69\n",
      "       motley-m       0.00      0.00      0.00         4\n",
      "         neal-s       0.70      0.64      0.67        67\n",
      "        nemec-g       0.71      0.85      0.78       179\n",
      "        panus-s       1.00      0.12      0.22         8\n",
      "        parks-j       0.80      0.74      0.77       166\n",
      "      pereira-s       1.00      0.27      0.43        22\n",
      "  perlingiere-d       0.98      0.94      0.96       174\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       1.00      0.33      0.50        21\n",
      "      platter-p       1.00      0.28      0.44        32\n",
      "       presto-k       0.62      0.77      0.69       287\n",
      "       quenet-j       0.00      0.00      0.00         2\n",
      "      quigley-d       0.92      0.79      0.85       150\n",
      "         rapp-b       1.00      0.29      0.45        31\n",
      "    reitmeyer-j       0.78      0.44      0.56        16\n",
      "       richey-c       0.92      0.77      0.84        74\n",
      "         ring-a       0.00      0.00      0.00         8\n",
      "         ring-r       1.00      0.07      0.12        15\n",
      "       rogers-b       0.88      0.56      0.68        27\n",
      "     ruscitti-k       1.00      0.18      0.31        22\n",
      "        sager-e       0.90      0.76      0.83       125\n",
      "        saibi-e       0.00      0.00      0.00        10\n",
      "    salisbury-h       0.94      0.39      0.55        41\n",
      "      sanchez-m       0.93      0.45      0.60        29\n",
      "      sanders-r       0.79      0.77      0.78       121\n",
      "     scholtes-d       0.80      0.32      0.46        37\n",
      "  schoolcraft-d       0.88      0.90      0.89       144\n",
      "    schwieger-j       0.84      0.71      0.77        45\n",
      "        scott-s       0.56      0.83      0.67       201\n",
      "    semperger-c       0.80      0.54      0.65        79\n",
      "   shackleton-s       0.92      0.90      0.91       190\n",
      "     shankman-j       1.00      0.22      0.36        18\n",
      "      shively-h       0.75      0.32      0.44        38\n",
      "     skilling-j       1.00      0.12      0.22        16\n",
      "      slinger-r       0.50      0.07      0.12        14\n",
      "        smith-m       0.90      0.77      0.83        70\n",
      "      solberg-g       1.00      0.12      0.22        16\n",
      "        staab-t       0.79      0.79      0.79        29\n",
      "      steffes-j       0.73      0.91      0.81       414\n",
      " stepenovitch-j       1.00      0.65      0.79        23\n",
      "       storey-g       1.00      0.54      0.70        41\n",
      "        sturm-f       0.91      0.59      0.72        49\n",
      "     swerzbin-m       1.00      0.19      0.32        21\n",
      "       taylor-m       0.77      0.74      0.76       164\n",
      "        tholt-j       0.89      0.65      0.75        74\n",
      "       thomas-p       1.00      0.43      0.61        46\n",
      "     townsend-j       0.00      0.00      0.00         5\n",
      "     tycholiz-b       0.82      0.75      0.78       106\n",
      "         ward-k       0.83      0.79      0.81       203\n",
      "       watson-k       0.79      0.88      0.83       290\n",
      "       weldon-c       0.97      0.70      0.82        54\n",
      "      whalley-g       0.67      0.10      0.17        20\n",
      "        white-s       0.91      0.91      0.91       134\n",
      "        whitt-m       0.85      0.57      0.68        91\n",
      "     williams-j       0.88      0.37      0.52        38\n",
      "    williams-w3       0.65      0.88      0.75       155\n",
      "        wolfe-j       0.95      0.64      0.77        28\n",
      "       ybarbo-p       1.00      0.49      0.65        35\n",
      "       zipper-a       0.39      0.51      0.44       103\n",
      "     zufferli-j       0.77      0.62      0.69       101\n",
      "\n",
      "       accuracy                           0.75     11299\n",
      "      macro avg       0.72      0.50      0.55     11299\n",
      "   weighted avg       0.78      0.75      0.74     11299\n",
      "\n",
      "\n",
      "Logistic Regression 31\n",
      "fitted\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.86      0.69      0.76       103\n",
      "       arnold-j       0.72      0.65      0.69       216\n",
      "        arora-h       1.00      0.15      0.26        20\n",
      "       badeer-r       0.00      0.00      0.00         2\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.70      0.68      0.69        73\n",
      "     baughman-d       1.00      0.21      0.35        28\n",
      "         beck-s       0.87      0.89      0.88       145\n",
      "       benson-r       0.00      0.00      0.00         5\n",
      "        blair-l       0.91      0.93      0.92       278\n",
      "      brawner-s       1.00      0.09      0.17        22\n",
      "          buy-r       0.91      0.84      0.87       117\n",
      "     campbell-l       0.79      0.32      0.46        34\n",
      "       carson-m       0.96      0.65      0.77        34\n",
      "         cash-m       0.84      0.88      0.86       164\n",
      "    causholli-m       0.98      0.88      0.93        67\n",
      "       corman-s       0.81      0.68      0.74       188\n",
      "     crandell-s       0.91      0.48      0.63        44\n",
      "       cuilla-m       1.00      0.34      0.51        41\n",
      "     dasovich-j       0.70      0.90      0.79       430\n",
      "        davis-d       1.00      0.21      0.35        14\n",
      "         dean-c       0.00      0.00      0.00        10\n",
      "     delainey-d       1.00      0.06      0.11        18\n",
      "      derrick-j       0.78      0.91      0.84       175\n",
      "       donoho-l       0.69      0.55      0.62        65\n",
      "      donohoe-t       0.00      0.00      0.00         9\n",
      "      dorland-c       0.82      0.85      0.83       176\n",
      "        ermis-f       0.00      0.00      0.00         5\n",
      "       farmer-d       0.91      0.64      0.75        77\n",
      "      fischer-m       1.00      0.11      0.19        19\n",
      "       forney-j       0.92      0.95      0.93       128\n",
      "         gang-l       1.00      0.63      0.77        27\n",
      "          gay-r       0.00      0.00      0.00         3\n",
      "     geaccone-t       0.82      0.69      0.75       109\n",
      "      germany-c       0.34      0.92      0.50       401\n",
      " gilbertsmith-d       0.00      0.00      0.00         4\n",
      "        giron-d       0.86      0.80      0.83        89\n",
      "     griffith-j       0.00      0.00      0.00        12\n",
      "      grigsby-m       0.65      0.74      0.69        87\n",
      "     haedicke-m       1.00      0.23      0.37        35\n",
      "     hayslett-r       0.58      0.86      0.69       159\n",
      "        heard-m       0.82      0.90      0.86       235\n",
      "  hendrickson-s       0.00      0.00      0.00        18\n",
      "    hernandez-j       1.00      0.21      0.35        19\n",
      "        hodge-j       0.00      0.00      0.00        29\n",
      "        holst-k       0.00      0.00      0.00        11\n",
      "       horton-s       1.00      0.54      0.70        46\n",
      "        hyatt-k       0.82      0.67      0.74        96\n",
      "        jones-t       0.88      0.68      0.77        94\n",
      "     kaminski-v       0.73      0.96      0.83       497\n",
      "         kean-s       0.68      0.68      0.68       137\n",
      "       keavey-p       0.00      0.00      0.00         9\n",
      "       keiser-k       0.88      0.82      0.85       109\n",
      "         king-j       0.00      0.00      0.00         4\n",
      "      kitchen-l       0.73      0.80      0.76       339\n",
      "   kuykendall-t       0.94      0.45      0.61        38\n",
      "     lavorato-j       0.52      0.62      0.57       201\n",
      "          lay-k       0.00      0.00      0.00         4\n",
      "      lenhart-m       0.59      0.83      0.69       230\n",
      "        lewis-a       0.00      0.00      0.00         7\n",
      "        lokay-m       0.80      0.36      0.49        45\n",
      "        lokey-t       0.94      0.42      0.58        36\n",
      "         love-p       0.83      0.83      0.83       178\n",
      "        lucci-p       0.68      0.63      0.65        70\n",
      "        maggi-m       0.96      0.66      0.78        35\n",
      "         mann-k       0.80      0.61      0.69        79\n",
      "       martin-t       0.76      0.65      0.70        68\n",
      "          may-l       0.00      0.00      0.00        13\n",
      "      mccarty-d       0.87      0.38      0.53        52\n",
      "    mcconnell-m       0.75      0.55      0.63        33\n",
      "        mckay-b       0.00      0.00      0.00        13\n",
      "        mckay-j       0.82      0.70      0.75        89\n",
      "   mclaughlin-e       0.88      0.69      0.77        32\n",
      "       meyers-a       0.00      0.00      0.00         3\n",
      "mims-thurston-p       0.95      0.57      0.71        69\n",
      "       motley-m       0.00      0.00      0.00         4\n",
      "         neal-s       0.62      0.66      0.64        67\n",
      "        nemec-g       0.71      0.83      0.77       179\n",
      "        panus-s       0.00      0.00      0.00         8\n",
      "        parks-j       0.78      0.71      0.74       166\n",
      "      pereira-s       1.00      0.14      0.24        22\n",
      "  perlingiere-d       0.98      0.92      0.95       174\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       1.00      0.24      0.38        21\n",
      "      platter-p       1.00      0.09      0.17        32\n",
      "       presto-k       0.63      0.74      0.68       287\n",
      "       quenet-j       0.00      0.00      0.00         2\n",
      "      quigley-d       0.92      0.79      0.85       150\n",
      "         rapp-b       1.00      0.10      0.18        31\n",
      "    reitmeyer-j       1.00      0.31      0.48        16\n",
      "       richey-c       0.90      0.74      0.81        74\n",
      "         ring-a       0.00      0.00      0.00         8\n",
      "         ring-r       1.00      0.07      0.12        15\n",
      "       rogers-b       0.90      0.33      0.49        27\n",
      "     ruscitti-k       1.00      0.05      0.09        22\n",
      "        sager-e       0.88      0.76      0.82       125\n",
      "        saibi-e       0.00      0.00      0.00        10\n",
      "    salisbury-h       1.00      0.39      0.56        41\n",
      "      sanchez-m       1.00      0.34      0.51        29\n",
      "      sanders-r       0.78      0.78      0.78       121\n",
      "     scholtes-d       0.79      0.30      0.43        37\n",
      "  schoolcraft-d       0.85      0.88      0.87       144\n",
      "    schwieger-j       0.86      0.56      0.68        45\n",
      "        scott-s       0.57      0.80      0.67       201\n",
      "    semperger-c       0.81      0.53      0.64        79\n",
      "   shackleton-s       0.94      0.89      0.92       190\n",
      "     shankman-j       0.00      0.00      0.00        18\n",
      "      shively-h       0.80      0.21      0.33        38\n",
      "     skilling-j       1.00      0.06      0.12        16\n",
      "      slinger-r       0.00      0.00      0.00        14\n",
      "        smith-m       0.87      0.79      0.83        70\n",
      "      solberg-g       1.00      0.06      0.12        16\n",
      "        staab-t       0.79      0.79      0.79        29\n",
      "      steffes-j       0.79      0.90      0.84       414\n",
      " stepenovitch-j       1.00      0.61      0.76        23\n",
      "       storey-g       1.00      0.46      0.63        41\n",
      "        sturm-f       0.91      0.59      0.72        49\n",
      "     swerzbin-m       1.00      0.05      0.09        21\n",
      "       taylor-m       0.75      0.74      0.75       164\n",
      "        tholt-j       0.84      0.64      0.72        74\n",
      "       thomas-p       1.00      0.39      0.56        46\n",
      "     townsend-j       0.00      0.00      0.00         5\n",
      "     tycholiz-b       0.81      0.75      0.78       106\n",
      "         ward-k       0.85      0.78      0.81       203\n",
      "       watson-k       0.80      0.87      0.83       290\n",
      "       weldon-c       0.97      0.70      0.82        54\n",
      "      whalley-g       0.67      0.10      0.17        20\n",
      "        white-s       0.90      0.91      0.91       134\n",
      "        whitt-m       0.82      0.58      0.68        91\n",
      "     williams-j       0.83      0.39      0.54        38\n",
      "    williams-w3       0.65      0.88      0.75       155\n",
      "        wolfe-j       1.00      0.57      0.73        28\n",
      "       ybarbo-p       0.94      0.43      0.59        35\n",
      "       zipper-a       0.48      0.44      0.46       103\n",
      "     zufferli-j       0.81      0.60      0.69       101\n",
      "\n",
      "       accuracy                           0.73     11299\n",
      "      macro avg       0.68      0.46      0.50     11299\n",
      "   weighted avg       0.77      0.73      0.72     11299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    #\"Naive Bayes\": MultinomialNB(alpha=1.0),\n",
    "    #\"Logistic Regression 4000\": LogisticRegression(max_iter=4000),\n",
    "    #\"Logistic Regression 2000\": LogisticRegression(max_iter=2000),\n",
    "    #\"Logistic Regression 1000\": LogisticRegression(max_iter=1000),\n",
    "    #\"Logistic Regression 500\": LogisticRegression(max_iter=500),\n",
    "    #\"Logistic Regression 250\": LogisticRegression(max_iter=250),\n",
    "    \"Logistic Regression 125\": LogisticRegression(max_iter=125),\n",
    "    \"Logistic Regression 62\": LogisticRegression(max_iter=62),\n",
    "    \"Logistic Regression 31\": LogisticRegression(max_iter=31),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(max_depth=20)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    #print(\"Confusion matrix:\")\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c359b-7bc3-430d-a189-21fbcb7b6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistical regression is the best. Preform For the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b42e65a-d811-41c4-8676-414ef0078eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression 1000000\n",
      "fitted\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.83      0.79      0.81        68\n",
      "       arnold-j       0.70      0.74      0.72       144\n",
      "        arora-h       0.67      0.14      0.24        14\n",
      "       badeer-r       0.00      0.00      0.00         1\n",
      "       bailey-s       0.00      0.00      0.00         3\n",
      "         bass-e       0.85      0.57      0.68        49\n",
      "     baughman-d       0.86      0.32      0.46        19\n",
      "         beck-s       0.88      0.86      0.87        96\n",
      "       benson-r       0.00      0.00      0.00         3\n",
      "        blair-l       0.92      0.92      0.92       186\n",
      "      brawner-s       1.00      0.27      0.42        15\n",
      "          buy-r       0.82      0.81      0.81        78\n",
      "     campbell-l       0.71      0.45      0.56        22\n",
      "       carson-m       1.00      0.74      0.85        23\n",
      "         cash-m       0.86      0.90      0.88       109\n",
      "    causholli-m       0.98      0.96      0.97        45\n",
      "       corman-s       0.76      0.66      0.71       125\n",
      "     crandell-s       0.89      0.53      0.67        30\n",
      "       cuilla-m       1.00      0.57      0.73        28\n",
      "     dasovich-j       0.71      0.88      0.79       287\n",
      "        davis-d       1.00      0.89      0.94         9\n",
      "         dean-c       1.00      0.17      0.29         6\n",
      "     delainey-d       1.00      0.25      0.40        12\n",
      "      derrick-j       0.81      0.88      0.84       117\n",
      "       donoho-l       0.86      0.58      0.69        43\n",
      "      donohoe-t       0.00      0.00      0.00         6\n",
      "      dorland-c       0.78      0.83      0.80       117\n",
      "        ermis-f       0.00      0.00      0.00         3\n",
      "       farmer-d       0.90      0.69      0.78        52\n",
      "      fischer-m       1.00      0.50      0.67        12\n",
      "       forney-j       0.93      0.92      0.92        86\n",
      "         gang-l       0.89      0.94      0.92        18\n",
      "          gay-r       0.00      0.00      0.00         2\n",
      "     geaccone-t       0.83      0.68      0.75        73\n",
      "      germany-c       0.46      0.92      0.61       267\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.82      0.63      0.71        59\n",
      "     griffith-j       1.00      0.12      0.22         8\n",
      "      grigsby-m       0.66      0.69      0.67        58\n",
      "     haedicke-m       1.00      0.22      0.36        23\n",
      "     hayslett-r       0.63      0.84      0.72       106\n",
      "        heard-m       0.78      0.87      0.82       156\n",
      "  hendrickson-s       0.00      0.00      0.00        12\n",
      "    hernandez-j       1.00      0.17      0.29        12\n",
      "        hodge-j       1.00      0.16      0.27        19\n",
      "        holst-k       0.00      0.00      0.00         7\n",
      "       horton-s       0.89      0.53      0.67        30\n",
      "        hyatt-k       0.81      0.67      0.74        64\n",
      "        jones-t       0.89      0.78      0.83        63\n",
      "     kaminski-v       0.76      0.97      0.85       332\n",
      "         kean-s       0.69      0.73      0.71        91\n",
      "       keavey-p       1.00      0.17      0.29         6\n",
      "       keiser-k       0.83      0.85      0.84        73\n",
      "         king-j       0.00      0.00      0.00         3\n",
      "      kitchen-l       0.76      0.80      0.78       226\n",
      "   kuykendall-t       0.84      0.62      0.71        26\n",
      "     lavorato-j       0.49      0.64      0.55       134\n",
      "          lay-k       0.00      0.00      0.00         3\n",
      "      lenhart-m       0.60      0.76      0.67       153\n",
      "        lewis-a       0.00      0.00      0.00         5\n",
      "        lokay-m       0.87      0.43      0.58        30\n",
      "        lokey-t       1.00      0.75      0.86        24\n",
      "         love-p       0.77      0.87      0.81       119\n",
      "        lucci-p       0.80      0.77      0.78        47\n",
      "        maggi-m       0.89      0.74      0.81        23\n",
      "         mann-k       0.89      0.65      0.76        52\n",
      "       martin-t       0.76      0.76      0.76        45\n",
      "          may-l       0.00      0.00      0.00         8\n",
      "      mccarty-d       0.88      0.44      0.59        34\n",
      "    mcconnell-m       0.92      0.50      0.65        22\n",
      "        mckay-b       0.75      0.33      0.46         9\n",
      "        mckay-j       0.83      0.73      0.77        59\n",
      "   mclaughlin-e       0.88      0.67      0.76        21\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.92      0.52      0.67        46\n",
      "       motley-m       0.00      0.00      0.00         3\n",
      "         neal-s       0.71      0.64      0.67        45\n",
      "        nemec-g       0.69      0.82      0.75       119\n",
      "        panus-s       1.00      0.60      0.75         5\n",
      "        parks-j       0.85      0.68      0.76       110\n",
      "      pereira-s       1.00      0.60      0.75        15\n",
      "  perlingiere-d       0.96      0.92      0.94       116\n",
      "       phanis-s       0.00      0.00      0.00         1\n",
      "      pimenov-v       0.75      0.21      0.33        14\n",
      "      platter-p       1.00      0.43      0.60        21\n",
      "       presto-k       0.64      0.77      0.70       191\n",
      "       quenet-j       0.00      0.00      0.00         1\n",
      "      quigley-d       0.94      0.83      0.88       100\n",
      "         rapp-b       1.00      0.14      0.25        21\n",
      "    reitmeyer-j       0.83      0.45      0.59        11\n",
      "       richey-c       0.89      0.78      0.83        50\n",
      "         ring-a       0.00      0.00      0.00         6\n",
      "         ring-r       1.00      0.10      0.18        10\n",
      "       rogers-b       0.86      0.67      0.75        18\n",
      "     ruscitti-k       1.00      0.21      0.35        14\n",
      "        sager-e       0.89      0.78      0.83        83\n",
      "        saibi-e       0.00      0.00      0.00         7\n",
      "    salisbury-h       0.94      0.59      0.73        27\n",
      "      sanchez-m       1.00      0.37      0.54        19\n",
      "      sanders-r       0.84      0.84      0.84        81\n",
      "     scholtes-d       0.89      0.64      0.74        25\n",
      "  schoolcraft-d       0.92      0.89      0.90        96\n",
      "    schwieger-j       0.94      0.57      0.71        30\n",
      "        scott-s       0.56      0.77      0.65       134\n",
      "    semperger-c       0.77      0.64      0.70        53\n",
      "   shackleton-s       0.93      0.91      0.92       127\n",
      "     shankman-j       1.00      0.08      0.15        12\n",
      "      shively-h       0.57      0.15      0.24        26\n",
      "     skilling-j       0.00      0.00      0.00        11\n",
      "      slinger-r       1.00      0.11      0.20         9\n",
      "        smith-m       0.86      0.77      0.81        47\n",
      "      solberg-g       1.00      0.09      0.17        11\n",
      "        staab-t       0.88      0.74      0.80        19\n",
      "      steffes-j       0.75      0.92      0.83       276\n",
      " stepenovitch-j       1.00      0.87      0.93        15\n",
      "       storey-g       0.91      0.71      0.80        28\n",
      "        sturm-f       0.78      0.55      0.64        33\n",
      "     swerzbin-m       1.00      0.14      0.25        14\n",
      "       taylor-m       0.77      0.79      0.78       109\n",
      "        tholt-j       0.91      0.59      0.72        49\n",
      "       thomas-p       0.94      0.55      0.69        31\n",
      "     townsend-j       0.00      0.00      0.00         3\n",
      "     tycholiz-b       0.81      0.73      0.77        71\n",
      "         ward-k       0.87      0.79      0.82       135\n",
      "       watson-k       0.77      0.92      0.84       194\n",
      "       weldon-c       1.00      0.64      0.78        36\n",
      "      whalley-g       0.67      0.14      0.24        14\n",
      "        white-s       0.93      0.90      0.91        89\n",
      "        whitt-m       0.83      0.64      0.72        61\n",
      "     williams-j       0.91      0.40      0.56        25\n",
      "    williams-w3       0.66      0.84      0.74       103\n",
      "        wolfe-j       1.00      0.74      0.85        19\n",
      "       ybarbo-p       0.89      0.70      0.78        23\n",
      "       zipper-a       0.47      0.49      0.48        69\n",
      "     zufferli-j       0.85      0.75      0.79        67\n",
      "\n",
      "       accuracy                           0.76      7533\n",
      "      macro avg       0.72      0.52      0.57      7533\n",
      "   weighted avg       0.78      0.76      0.75      7533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/WAVE/users2/unix/hstrauss/.conda/envs/140wi25/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models= {\"Logistic Regression 100000000\": LogisticRegression(max_iter=100)}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    #print(\"Confusion matrix:\")\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17db4a8-a791-4733-85dd-6c6001da35c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "140wi25",
   "language": "python",
   "name": "140wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
